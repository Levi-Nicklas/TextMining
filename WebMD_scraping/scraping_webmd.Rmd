---
title: "Testing web scraping for WebMD message boards"
output: html_notebook
---

Jump to [solution section](#solved) for final approach

```{r}
library(rvest)
library(tidyverse)
library(purrr)
```

Page 1

```{r}
read_html("https://messageboards.webmd.com/health-conditions/f/heart-health") %>%
  html_nodes('.thread-body') %>%
  html_text()
```

Page 2

```{r}
read_html("https://messageboards.webmd.com/health-conditions/f/heart-health#pi37=3") %>%
  html_nodes('.thread-body') %>%
  html_text()
```

Same page. DID NOT WORK


```{r}
df_webmd <- lapply(paste0('https://messageboards.webmd.com/health-conditions/f/heart-health#pi37=', 2:10),
                function(url){
                    url %>% read_html() %>% 
                        html_nodes(".thread-body") %>% 
                        html_text()
                })
```

Same problem.


Read manually. Pages 18 and 19 were downloaded and place in current working directory. 
Inefficient but does the job. Should we keep doing this? or simply keep this idea of data source for future reference?

```{r}
myp <- read_html("page18.htm") %>%
 html_nodes('.thread-body') %>%
  html_text()
```


```{r}
myq <- read_html("page19.htm") %>%
 html_nodes('.thread-body') %>%
  html_text()
```

Check:

```{r}
# myp
# myq
```


Do we need to use `RSelenium`???? `splashr`????

<https://community.rstudio.com/t/scraping-table-weirdness-with-rvest-undesired-xml-nodeset-0/6798/7>


## (SOLVED) New Test: 2020/03/30 {#solved}


```{r}
paste0('https://messageboards.webmd.com/health-conditions/f/heart-health#pi37=', 2:10)
```

**need to change `=` by `?` in web address**


```{r}
page <- "https://messageboards.webmd.com/health-conditions/f/heart-health?pi37=82"

#df <- 
page %>% 
  read_html() %>% 
    html_nodes('.forum-thread') %>%    # select enclosing nodes
    # iterate over each, pulling out desired parts and coerce to data.frame
    map_df(~list(
      title = html_nodes(.x, '.unread') %>%
        html_text() %>% 
        {if(length(.) == 0) NA else .}, 
      comment = html_nodes(.x, '.thread-body') %>% 
        html_text() %>% 
        {if(length(.) == 0) NA else .},
      tags = html_nodes(.x, '.tag-list') %>% 
        html_text() %>% 
        {if(length(.) == 0) NA else .},    # replace length-0 elements with NA
      date = html_nodes(.x, '.thread-ago') %>% 
        html_text() %>%    
        {if(length(.) == 0) NA else .}
                 )
           )
  
```

The function below creates a dataframe with title, comment, tags, and date for every post in any of the feasible pages. 

```{r}
get_post_df <- function(page){
    df <- read_html(page) %>% 
    html_nodes('.forum-thread') %>%    # select enclosing nodes
    # iterate over each, pulling out desired parts and coerce to data.frame
    map_df(~list(
      title = html_nodes(.x, '.unread') %>%
        html_text() %>% 
        {if(length(.) == 0) NA else .}, 
      comment = html_nodes(.x, '.thread-body') %>% 
        html_text() %>% 
        {if(length(.) == 0) NA else .},
      tags = html_nodes(.x, '.tag-list') %>% 
        html_text() %>% 
        {if(length(.) == 0) NA else .},    # replace length-0 elements with NA
      date = html_nodes(.x, '.thread-ago') %>% 
        html_text() %>%    
        {if(length(.) == 0) NA else .}
                 )
           )
  return(df)
}
```


For example: 

```{r}
my_page_1 <- "https://messageboards.webmd.com/health-conditions/f/heart-health?pi37=1"
get_post_df(my_page_1)
```


- Need to iterate from page 1 to page 82

What do we need? Programmatically create every page, get the dataframe for each page, and do a final `bind_rows()` type of operation.

Get all possible pages: 

```{r}
# test with first 3
#pages <- 1:82
pages <- 1:3
base_url <- "https://messageboards.webmd.com/health-conditions/f/heart-health?pi37="
urls <- paste0(base_url, pages)
```

Test:

```{r}
map_df(urls, get_post_df)
```
 
It seems to work. 
All pages: 


```{r}
pages <- 1:82
base_url <- "https://messageboards.webmd.com/health-conditions/f/heart-health?pi37="
urls <- paste0(base_url, pages)
```

```{r}
read_html("https://messageboards.webmd.com/health-conditions/f/heart-health?pi37=10") %>% class()
```


Get them all:

```{r}
big_webmd_df <- map_df(urls, get_post_df)
```



Save data frame as a `.csv` file: 

```{r}
write_csv(big_webmd_df, "big_webmd_df.csv")
```

_used for preliminary development_



(possible) Helper functions:


```{r, eval = F}
get_title <- function(url){
  url %>% 
    read_html() %>% 
    html_nodes('.unread') %>% 
    html_text()
  
}


get_comment <- function(url){
  url %>% 
    read_html() %>% 
    html_nodes('.thread-body') %>% 
    html_text()
  
}

get_date <- function(url){
  url %>% 
    read_html() %>% 
    html_nodes('.thread-ago') %>% 
    html_text()
  
}
```


